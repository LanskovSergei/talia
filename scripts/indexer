import os
import requests
import re
from llama_index import VectorStoreIndex, Document, StorageContext
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

# ======= ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ =======
OUTLINE_API_URL = "https://outline.taliaslimbot.com/api"
OUTLINE_API_KEY = "ol_"
OPENAI_API_KEY = "sk-"

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
HEADERS = {"Authorization": f"Bearer {OUTLINE_API_KEY}"}


def clean_markdown(text: str) -> str:
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ markdown-Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ¸ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)  # ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)   # ÑÑÑ‹Ğ»ĞºĞ¸
    text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)  # Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
    text = re.sub(r'\*\*|\*|__|_', '', text)     # Ğ¶Ğ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¸ ĞºÑƒÑ€ÑĞ¸Ğ²
    text = re.sub(r'> ', '', text)               # Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ñ‹
    text = re.sub(r'-{2,}', '', text)            # Ğ»Ğ¸Ğ½Ğ¸Ğ¸
    text = re.sub(r'[ğŸ”¹ğŸ”¸ğŸ“ğŸ”¥ğŸ›‘ğŸ¯âœ…â¡ï¸â†’â­ğŸš«ğŸ¥¦ğŸ«â˜•ğŸ˜ŒğŸ˜ŠğŸ¤–ğŸ’¬ğŸ§ ğŸ“„ğŸ“ğŸ”]', '', text)
    return text.strip()


# ======= ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² =======
def fetch_all_documents():
    print("ğŸ“ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¹...")
    collections = requests.post(
        f"{OUTLINE_API_URL}/collections.list",
        headers=HEADERS
    ).json().get("data", [])
    print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¹: {len(collections)}")

    all_docs = []

    for col in collections:
        col_id = col["id"]
        col_name = col["name"]
        print(f"ğŸ” ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ: {col_name} ({col_id})")

        page = 0
        while True:
            resp = requests.post(
                f"{OUTLINE_API_URL}/documents.list",
                headers=HEADERS,
                json={"collectionId": col_id, "limit": 100, "offset": page * 100}
            )
            results = resp.json().get("data", [])
            if not results:
                break

            for doc in results:
                doc_id = doc["id"]
                title = doc["title"]

                export_resp = requests.post(
                    f"{OUTLINE_API_URL}/documents.export",
                    headers=HEADERS,
                    json={"id": doc_id}
                )
                if export_resp.status_code != 200:
                    print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°: {title}")
                    continue

                raw_text = export_resp.json().get("data", "").strip()
                if not raw_text:
                    print(f"âš ï¸ Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹: {title}")
                    continue

                full_text = f"{title}\n\n{clean_markdown(raw_text)}"

                all_docs.append(Document(
                    text=full_text,
                    metadata={"title": title, "collection": col_name}
                ))

            page += 1

    print(f"ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: {len(all_docs)}")
    return all_docs


# ======= ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° =======
def build_index(docs):
    llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
    embed_model = OpenAIEmbedding(model="text-embedding-3-small")
    index = VectorStoreIndex.from_documents(docs, llm=llm, embed_model=embed_model)
    index.storage_context.persist(persist_dir="./storage")
    return index


# ======= Ğ¢Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ° =======
if __name__ == "__main__":
    print("ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Outline...")
    documents = fetch_all_documents()

    if documents:
        print("âš™ï¸ Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ¸Ğ½Ğ´ĞµĞºÑ...")
        build_index(documents)
        print("âœ… Ğ˜Ğ½Ğ´ĞµĞºÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ² ./storage")
    else:
        print("âŒ ĞĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸.")
